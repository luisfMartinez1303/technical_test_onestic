import json
import os

from modules.SEO_agent import SEO_agent
from dotenv import load_dotenv 
from langsmith.evaluation import evaluate
from langsmith.schemas import Example, Run

load_dotenv('.env')

def calculate_average_scores(json_string: str) -> dict:
    """
    Calculates the average score from a JSON string containing evaluation scores.

    Args:
        json_string (str): JSON-formatted string containing evaluation scores.

    Returns:
        dict: Dictionary with the key "key" set to "average" and the calculated average score.
    """
    data = json.loads(json_string)
    scores = data.get("evaluation", {}).values()
    
    if not scores:
        return {"key": "average", "score": 0}
    
    average = sum(scores) / len(scores)
    
    return {"key": "average", "score": average}

def langsmith_app(inputs: dict) -> dict:
    """
    Generates an SEO specification sheet based on the product data provided in the inputs.

    Args:
        inputs (dict): Dictionary containing product data.

    Returns:
        dict: Dictionary with the generated SEO specification sheet.
    """
    seo_agent = SEO_agent()
    output = seo_agent.generate_seo_spec_sheet(inputs["product_data"])
    return {"output": output}

def langsmith_app2(inputs: dict) -> dict:
    """
    Reads the content of an image from a URL provided in the inputs.

    Args:
        inputs (dict): Dictionary containing the image URL.

    Returns:
        dict: Dictionary with the content extracted from the image.
    """
    seo_agent = SEO_agent()
    output = seo_agent.read_images(inputs["image_url"])
    return {"output": output}

def evaluation_model_response(run: Run, example: Example) -> dict:
    """
    Evaluates the SEO specification sheet generated by the model and calculates the average score.

    Args:
        run (Run): Object containing the outputs of the model run.
        example (Example): Object containing the example inputs for the evaluation.

    Returns:
        dict: Dictionary with the average score of the evaluation.
    """
    seo_agent = SEO_agent()
    json_score = seo_agent.evaluation(json_data=example.inputs['product_data'], output=run.outputs["output"])
    json_score = calculate_average_scores(json_score)
    
    return json_score

def evaluation_model_response_read_image(run: Run, example: Example) -> dict:
    """
    Evaluates the content extracted from an image by the model and calculates the average score.

    Args:
        run (Run): Object containing the outputs of the model run.
        example (Example): Object containing the example inputs for the evaluation.

    Returns:
        dict: Dictionary with the average score of the evaluation.
    """
    seo_agent = SEO_agent()
    json_score = seo_agent.evaluation(output=run.outputs["output"], url=example.inputs['image_url'], eval=False)
    json_score = calculate_average_scores(json_score)
    
    return json_score

def evaluate_db(eval: bool = True):
    """
    Evaluates the database based on the specified mode (SEO spec sheet or image reading).

    Args:
        eval (bool): Determines which evaluation function to use. If True, evaluates SEO spec sheet; otherwise, evaluates image reading.
    """
    if eval:
        experiment_results = evaluate(
            langsmith_app, 
            data="generate_seo_spec_sheet", 
            evaluators=[evaluation_model_response], 
            experiment_prefix=os.getenv("ID_GENERATE_SEO_SPEC_SHEET_PROMPT")
        )
    else:
        experiment_results = evaluate(
            langsmith_app2, 
            data="read_images", 
            evaluators=[evaluation_model_response_read_image], 
            experiment_prefix=os.getenv("ID_READ_IMAGES_PROMPT")
        )

if __name__ == "__main__":
    evaluate_db()